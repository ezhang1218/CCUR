{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc832edf-d39a-4a1b-9d4c-951852469461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from CFS_SG import CFS_SG\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bcbffd-3b65-44c0-9eaf-3b2c3aef67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "from scipy.linalg import pinv\n",
    "from itertools import product\n",
    "import numpy as np, h5py, os\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter \n",
    "from scipy.sparse import vstack, coo_matrix, csc_matrix, isspmatrix_csc\n",
    "%matplotlib inline\n",
    "import scanpy as sc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4708c5-4945-4fe7-a1aa-2d536747f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"small_molecule_fore.csv\", index_col = 0)\n",
    "background = pd.read_csv(\"small_molecule_back.csv\", index_col = 0)\n",
    "gene_names = target.columns.to_numpy()\n",
    "cell_names = target.index.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea35df86-def3-432a-9ce4-c4fa3cde63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = np.concatenate([np.zeros(background.shape[0]), np.ones(target.shape[0])])\n",
    "data_train = np.concatenate([background, target])\n",
    "\n",
    "data_train_tensor = torch.tensor(data_train, dtype=torch.float32)  \n",
    "labels_train_tensor = torch.tensor(labels_train, dtype=torch.long) \n",
    "\n",
    "dataset = TensorDataset(data_train_tensor, labels_train_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9312e23-2d9b-4bd0-8aa4-e471b8462374",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = target.shape[1]\n",
    "output_size = background.shape[1]\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00ea2f4-620c-4988-8b97-b98fe82a32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CFS_SG(\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    hidden=[512, 512], # Number of units in each hidden layer\n",
    "    k_prime=20, # Background dimension size\n",
    "    lam=0.175, # Tuned to select about 10 features\n",
    "    lr=1e-3,\n",
    "    loss_fn=nn.MSELoss()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c49e873-00ad-405b-a748-6a25f76435d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/nas/longleaf/home/eyzhang/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB MIG 2g.10gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-f431a4cc-e3b9-502a-a0e9-8bb3d21b4074]\n",
      "\n",
      "  | Name                   | Type       | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | background_input_layer | Sequential | 130 K  | train\n",
      "1 | fc                     | Sequential | 1.3 M  | train\n",
      "2 | loss_fn                | MSELoss    | 0      | train\n",
      "  | other params           | n/a        | 1.0 K  | n/a  \n",
      "--------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.720     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/nas/longleaf/home/eyzhang/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d697c97f56234c5091e10f324290d102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/eyzhang/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: You called `self.log('num_selected_gates', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'num_selected_gates': ...})` instead.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator='gpu', devices=1)\n",
    "trainer.fit(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd3ae2d-320d-4674-877e-6934219f37bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-f431a4cc-e3b9-502a-a0e9-8bb3d21b4074]\n",
      "\n",
      "  | Name                   | Type       | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | background_input_layer | Sequential | 130 K  | train\n",
      "1 | fc                     | Sequential | 1.3 M  | train\n",
      "2 | loss_fn                | MSELoss    | 0      | train\n",
      "  | other params           | n/a        | 1.0 K  | n/a  \n",
      "--------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.720     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81adc7a565614b91884dbf169a49df60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, accelerator='gpu', devices=1)\n",
    "trainer.fit(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597e20f3-86ea-4b7b-b8f5-cadc47a79509",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = model.get_inds(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626fbbe8-a2a3-444f-9590-e196ec850735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ENSG00000169715', 'ENSG00000198959', 'ENSG00000245532',\n",
       "       'ENSG00000196754', 'ENSG00000168036', 'ENSG00000123689',\n",
       "       'ENSG00000197061', 'ENSG00000135679', 'ENSG00000169429',\n",
       "       'ENSG00000205426'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_names[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e324778-cda0-4dda-bf91-25224192135b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
