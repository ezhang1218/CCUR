{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9149ad-7824-4309-a45f-d67845a06765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from data import LabeledDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from CFS_SG import CFS_SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2768f89-6b2d-46fc-8064-1634857cc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "from scipy.linalg import pinv\n",
    "from itertools import product\n",
    "import numpy as np, h5py, os\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter \n",
    "from scipy.sparse import vstack, coo_matrix, csc_matrix, isspmatrix_csc\n",
    "%matplotlib inline\n",
    "import scanpy as sc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec64921-da4f-4693-96ee-3cbe7c7a5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_binary_file(file_url: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Download binary data file from a URL.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        file_url: URL where the file is hosted.\n",
    "        output_path: Output path for the downloaded file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None.\n",
    "    \"\"\"\n",
    "    request = requests.get(file_url)\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(request.content)\n",
    "    print(f\"Downloaded data from {file_url} at {output_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "def download_haber_2017(output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Download Haber et al. 2017 data from the hosting URLs.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        output_path: Output path to store the downloaded and unzipped\n",
    "        directories.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None. File directories are downloaded to output_path.\n",
    "    \"\"\"\n",
    "\n",
    "    url = (\n",
    "        \"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE92nnn/GSE92332/suppl/GSE92332\"\n",
    "        \"_SalmHelm_UMIcounts.txt.gz\"\n",
    "    )\n",
    "\n",
    "    output_filename = os.path.join(output_path, url.split(\"/\")[-1])\n",
    "\n",
    "    download_binary_file(url, output_filename)\n",
    "def read_haber_2017(file_directory: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the expression data from Haber et al. 2017 given the directory.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        file_directory: Directory containing Haber et al. 2017 data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A DataFrame containing single-cell gene expression counts, with cell\n",
    "        identification barcodes as column names and gene IDs as indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Path to the file\n",
    "    file_path = os.path.join(file_directory, \"GSE92332_SalmHelm_UMIcounts.txt.gz\")\n",
    "    \n",
    "    # Read the .txt.gz file, assuming the first column contains gene names\n",
    "    with gzip.open(file_path, \"rt\") as f:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", index_col=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_haber_2017(download_path: str, n_top_genes: int) -> (AnnData, list):\n",
    "    \"\"\"\n",
    "    Preprocess expression data from Haber et al. 2017.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        download_path: Path containing the downloaded Haber et al. 2017 data file.\n",
    "        n_top_genes: Number of most variable genes to retain.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        An AnnData object containing single-cell expression data. The layer\n",
    "        \"count\" contains the count data for the most variable genes. The X\n",
    "        variable contains the total-count-normalized and log-transformed data\n",
    "        for the most variable genes (a copy with all the genes is stored in\n",
    "        .raw).\n",
    "        A list of conditions associated with each cell.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the expression data\n",
    "    df = read_haber_2017(download_path)\n",
    "    \n",
    "    # Transpose the dataframe so that genes become columns and cells are rows\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Extract metadata from the cell names\n",
    "    cell_groups = []\n",
    "    barcodes = []\n",
    "    conditions = []\n",
    "    cell_types = []\n",
    "    cell_names = []\n",
    "\n",
    "    for cell in df.index:\n",
    "        try:\n",
    "            cell_group, barcode, condition, cell_type = cell.split(\"_\")\n",
    "            cell_groups.append(cell_group)\n",
    "            barcodes.append(barcode)\n",
    "            conditions.append(condition)\n",
    "            cell_types.append(cell_type)\n",
    "            cell_names.append(cell)\n",
    "        except ValueError:\n",
    "            print(f\"Error parsing cell name: {cell}\")\n",
    "            continue\n",
    "\n",
    "    # Create a DataFrame for metadata\n",
    "    metadata_df = pd.DataFrame(\n",
    "        {\n",
    "            \"cell_group\": cell_groups,\n",
    "            \"barcode\": barcodes,\n",
    "            \"condition\": conditions,\n",
    "            \"cell_type\": cell_types,\n",
    "        }, index=cell_names  # Ensure the cell names are set as the index\n",
    "    )\n",
    "\n",
    "    # Create the AnnData object with gene names in 'var' and cell metadata in 'obs'\n",
    "    adata = AnnData(X=df.values, obs=metadata_df, var=pd.DataFrame(index=df.columns))\n",
    "\n",
    "    # Preprocess the data: normalize, log-transform, and keep most variable genes\n",
    "    adata = adata[adata.obs[\"condition\"] != \"Hpoly.Day3\"]\n",
    "    adata.layers[\"count\"] = adata.X.copy()\n",
    "\n",
    "    # Normalize and log-transform\n",
    "    sc.pp.normalize_total(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "    adata.raw = adata\n",
    "\n",
    "    # Select highly variable genes\n",
    "    sc.pp.highly_variable_genes(\n",
    "        adata, flavor=\"seurat_v3\", n_top_genes=n_top_genes, layer=\"count\", subset=True\n",
    "    )\n",
    "\n",
    "    # Remove cells with all zero counts\n",
    "    adata = adata[adata.layers[\"count\"].sum(1) != 0]  # Remove cells with all zeros.\n",
    "\n",
    "    # Return the AnnData object and the conditions list\n",
    "    return adata, conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6359e3-dce4-4e9d-8e7e-9ebd507a583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data from https://ftp.ncbi.nlm.nih.gov/geo/series/GSE92nnn/GSE92332/suppl/GSE92332_SalmHelm_UMIcounts.txt.gz at GSE92332_SalmHelm_UMIcounts.txt.gz\n"
     ]
    }
   ],
   "source": [
    "root_data_path = \"\"   # local computer data path to downloaad the dataset. \n",
    "\n",
    "download_haber_2017(root_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1e6e79-ac75-400d-9ad5-e011fe585567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2379908/3766862134.py:132: ImplicitModificationWarning: Setting element `.layers['count']` of view, initializing view as actual.\n",
      "  adata.layers[\"count\"] = adata.X.copy()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data, conditions = preprocess_haber_2017(root_data_path, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b232683-c6ad-4180-a8d1-ce0e44db1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground = data[data.obs[\"condition\"] != \"Control\"]\n",
    "k, p = foreground.shape\n",
    "\n",
    "\n",
    "background = data[data.obs[\"condition\"] == \"Control\"]\n",
    "m, p = background.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a8fe0e-84f9-4b57-8739-7b28622b4aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground = foreground.to_df()\n",
    "background = background.to_df()\n",
    "genes = foreground.columns.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1844c-f0b7-443e-9e02-15e209956a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1505f2d-4d31-4243-ab87-b3a2a9bb03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label here determines target versus background\n",
    "labels_train = np.concatenate([np.zeros(background.shape[0]), np.ones(foreground.shape[0])])\n",
    "data_train = np.concatenate([background, foreground])\n",
    "\n",
    "# Ensure your data is converted to the correct dtype outside the dataset class\n",
    "data_train = torch.from_numpy(data_train).float()  # Convert data to torch float32\n",
    "labels_train = torch.from_numpy(labels_train).float()  # Convert labels to torch float32\n",
    "\n",
    "# Create the dataset as before\n",
    "dataset = LabeledDataset(data_train.numpy(), labels_train.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d81941-c41e-469e-a1a7-8be59b03f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = foreground.shape[1]\n",
    "output_size = background.shape[1]\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f532601c-c072-4001-a6f1-2ec525ad6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CFS_SG(\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    hidden=[512, 512], # Number of units in each hidden layer\n",
    "    k_prime=20, # Background dimension size\n",
    "    lam=0.15, # Tuned to select about 10 features\n",
    "    lr=1e-3,\n",
    "    loss_fn=nn.MSELoss()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "242cd461-3670-49ae-a0cd-1338998a51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c45ee9ee-f6d1-4bf7-8b52-ec9d19a4fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/nas/longleaf/home/eyzhang/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB MIG 2g.10gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-f431a4cc-e3b9-502a-a0e9-8bb3d21b4074]\n",
      "\n",
      "  | Name                   | Type       | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | background_input_layer | Sequential | 130 K  | train\n",
      "1 | fc                     | Sequential | 1.3 M  | train\n",
      "2 | loss_fn                | MSELoss    | 0      | train\n",
      "  | other params           | n/a        | 1.0 K  | n/a  \n",
      "--------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.720     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972e18e6a11d4479b85948e868266f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/longleaf/home/eyzhang/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: You called `self.log('num_selected_gates', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'num_selected_gates': ...})` instead.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, accelerator='gpu', devices=1)\n",
    "trainer.fit(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7035defb-d6a0-4a1f-924c-649570043697",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = model.get_inds(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4922174e-d956-45ac-ad4d-3a6a5df6f239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['H2.Aa', 'Cd74', 'Ang4', 'H2.Ab1', 'Ifitm3', 'Uqcrb', 'S100a6',\n",
       "       'Mt2', 'Fabp6', 'Reg1'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genes[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a03d45-ef08-4e4c-ba84-fa1084d2ee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631cbe6e-e5a2-4dc9-a6d5-57f7cc1082d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
